# Ruwance - WRO 2025<img width="40" alt="robotek" src="https://github.com/user-attachments/assets/bffadef9-b0aa-4810-93ff-13db445ac044" /><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Flag_of_Peru_%28state%29.svg/1200px-Flag_of_Peru_%28state%29.svg.png" alt="Peru Flag" width="30"/>

<p align="center">
  <img src="https://github.com/user-attachments/assets/fbcddbcc-7a4a-4e44-bd8f-0d2e3b5a059e" alt="Banner Ruwance" width="100%">
</p>

[![Instagram](https://img.shields.io/badge/Instagram-%23FFA500.svg?style=for-the-badge&logo=Instagram&logoColor=white&style=plastic)](https://www.instagram.com/robotekperu/)
[![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white&style=plastic)](https://www.youtube.com)
[![Facebook](https://img.shields.io/badge/Facebook-%232B25CF.svg?style=for-the-badge&logo=Facebook&logoColor=white&style=plastic)](https://www.facebook.com/p/Robotek-Per%C3%BA-61566493439700/)


+ Team Name: **Ruwance**
+ Car Name: **Riska**
+ Club Name: **Robotek**
+ Team Members: **Isabella Gonzales & Vania Pachas**
+ Coach: **Anthony Valladolid**

---

*Welcome! üéâ* <br>
We are Team Ruwance, proudly representing Per√∫ at the 2025 World Robot Olympiad! This GitHub repository contains the documentation, code, and full development journey of our autonomous vehicle, designed and built to compete in the Future Engineers Challenge.



# Repository Structure
* `t-photos` team photos (official one and funny one)
* `v-photos` photos of our vehicle (from every side, from top and bottom)
* `video` demonstration video link of the robot in the challenges
* `schemes` Wiring diagram with pins and electronic components
* `src` code of the robotic vehicle system
* `models` 3D printed parts and other chassis pieces 
* `other` Extra documentation


# Table of Contents
1. [Meet the Team!](#1-meet-the-team)
   
3. [Vehicle Overview](#2-vehicle-overview)
   + [General Description of the Car](#general-description-of-the-car)
   + [Versions of the Car](#versions-of-the-car)
     
4. [System Setup](#3-system-setup)
   + [Operating Environment Overview](#operating-environment-overview)
   + [ROS](#ros)
   + [Ubuntu](#ubuntu)
   + [Raspberry Pi](#raspberry-pi)
     
5. [Mobility Management](#4-mobility-management)
   + [Steering System - Ackermann](#steering-system-ackermann)
   + [Motor and Drivetrain](#motor-and-drivetrain)
   + [3D Pieces](#3d-pieces)
   
6. [Power & Sense Management](#5-power--sense-management)
   + [Power Source](#power-source)
   + [Sensors Integration](#sensors-integration)
   + [BOM (Bill of Materials)](#bom-bill-of-materials)
   + [Wiring Diagram](#wiring-diagram)

7. [Obstacle Management](#6-obstacle-management)
   + [Autonomous Driving Logic](#autonomous-driving-logic)
   + [Open Challenge](#open-challenge)
   + [Obstacle Challenge](#obstacle-challenge)
     
8. [Assembly Instructions](#7-assembly-instructions)
9. [Performance Videos](#8-performance-videos)
   
<br>

# Main Content

## 1. Meet the Team!
<table>
  <tr>
    <td width="55%" valign="top">
      <strong>Vania Pachas</strong><br><br>
      <img width="15" alt="pencial" src="https://github.com/user-attachments/assets/cf4faf62-f43d-47b9-8fd1-917cc4955a78" /> <i>Responsible for Mechanical Design & Technical documentation</i><br>
       <img width="15" alt="mail" src="https://www.clipartmax.com/png/full/278-2785632_big-image-mail-icon-png-circle.png" /> vaniaapachas@gmail.com<br>
       <img width="15" alt="location" src="https://github.com/user-attachments/assets/11318a7b-9411-4503-9885-926fe4fb4ffb" /> Lima, Per√∫<br><br>
      Hi!! I'm Vania, an 18-year-old Peruvian passionate about innovation and STEM education. This is my first time in the WRO, and I'm excited to learn and take on this new challenge! I also love dancing salsa, crocheting, watching romcoms, and I‚Äôm a Quantum Computing enthusiast :)<br><br>
   
  <td width="45%" align="center">
    <img src="https://github.com/user-attachments/assets/36a0b27f-ee27-486c-8914-90469e85a03d" alt="Vania"/>
    </td>
  </tr>
  
</table>

<table>
  <tr>
    <td width="55%" valign="top">
      <strong>Isabella Gonzales</strong><br><br>
      <img width="15" alt="pencial" src="https://github.com/user-attachments/assets/cf4faf62-f43d-47b9-8fd1-917cc4955a78" /> <i>Responsible for Electronics and Technical Designer documentation</i><br>
       <img width="15" alt="mail" src="https://www.clipartmax.com/png/full/278-2785632_big-image-mail-icon-png-circle.png" /> isabellamilagros842@gmail.com<br>
       <img width="15" alt="location" src="https://github.com/user-attachments/assets/11318a7b-9411-4503-9885-926fe4fb4ffb" /> Lima, Per√∫<br><br>
      Hello! My name is Isabella, I'm 16 years old, and I love robotics. I founded Robotek Per√∫, a club where students can learn robotics and join competitions, and this is my second time at the WRO. My favorite hobbies are singing with my choir, practicing taekwondo,¬†and¬†art. <br><br>
   
  <td width="45%" align="center">
    <img src="https://github.com/user-attachments/assets/07735d5c-00a2-43e7-8e0f-f4cf62ae2247" alt="Isa"/>
    </td>
  </tr>
  
</table>

<br>

> [!NOTE]
> ü§îWhy is our team named *Ruwance*?<br><br>
> The name *Ruwance* comes from combining the Quechua word "**ruway**" and the English word "**chance**". Ruway means "to create" or "to do", which really reflects who we are, a team that is always making, building, and experimenting. As students of Robotek from Lima, Peru, we have learned that creativity and resilience are important traits to succeed in and enjoy our robotics journey.

<br>


## 2. Vehicle Overview
<p align="center">
  <img src="https://github.com/user-attachments/assets/105155b6-ddb2-4885-aa63-7e07f1468315" alt="Vehicle Overview" width="100%">
</p>

### **<ins>General Description of the Car</ins>**
Our autonomous vehicle is built to take on both the Open and Obstacle Challenges at the Future Engineers competition. Running on Ubuntu with ROS, it can process information and make decisions in real time. The car uses an Ackermann steering system and a custom-built chassis to move smoothly through turns and straight paths. A LiDAR sensor helps it detect the field walls, while the camera identifies traffic signs and obstacles. With this setup, the car can adapt its route, count laps, and complete the course efficiently.
<br>

| Front | Left | Right |
| :--: | :--: | :--: | 
| <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="90%" /> | <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="85%" /> |  <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="85%" /> | 

| Back  | Top  | Bottom |
| :--: | :--: |:--: |  
| <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="90%" /> | <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="85%" /> |  <img src="https://github.com/user-attachments/assets/63ed0743-32e4-4605-b754-b6c322b21799" width="85%" /> | 
> [!NOTE]
> Visit our [`v-photos`](https://github.com/vania020/wro2025-robotek/tree/main/v-photos) folder for more detailed photos of the car üì∏üöó


### **<ins>Versions of the Car</ins>**
Our vehicle has gone through **10 versions** (and since we are always improving, there are still more to come!). It all started with a cardboard prototype, then moved on to acrylic and metal chassis designs, and later, we made personalized adjustments to a HiWonder kit. Along the way, we also experimented with different wheel designs, repositioned components, tested sensors like LiDAR, and finally adopted a new operating environment with ROS and Ubuntu.
<table>  
  <tr>
    <th width="10%">Version</th>
    <th width="40%">Car Photo</th>
    <th width="50%">Description</th>
  </tr>
  
  <!-- Version 1 -->
  <tr>
    <td align="center"><i>Version N¬∞1</i></td>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/88cb9f71-715d-41d5-8dcd-54870af7fa87" width="330"height="230"/>
    </td>
    <td>
      We built a cardboard prototype to test and understand the Ackermann steering system and wheel movement. This served as a reference to learn about the placement of components and systems.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-1">‚û°Ô∏è See more photos üöó</a>
    </td>
  </tr>

  <!-- Version 2 -->
  <tr>
    <td align="center"><i>Version N¬∞2</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/14afa30d-317e-43f8-b6ef-d63897212c88" width="330" height="230"/>
    </td>
    <td>
      <br>We cut and incorporated an acrylic chassis and designed/3D-printed housing pieces for the camera, Ackermann system, and other components. The Ackermann used a stepper motor, and to perceive its surroundings, the car relied on infrared sensors and a webcam. 
      <br><br>The main controllers were a Raspberry Pi 4 and an Arduino Nano, powered by a power bank and lithium batteries. The car‚Äôs movement was driven by a single motor.<br><br> 
      <a href="v-photos/vehicle-versions/README.md#version-2">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 3 -->
  <tr>
    <td align="center"><i>Version N¬∞3</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/5299a922-0fb3-43a8-9a32-9bdbc77225d7" width="330" height="230"/>
    </td>
    <td>
      <br> We replaced the power bank with a smaller one, adjusted component placement into a two-level car system, and installed new wheels. Lithium batteries were replaced with higher-current ones, and the infrared sensors were moved to the front, so the vehicle could make more precise turns.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-3">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 4 -->
  <tr>
    <td align="center"><i>Version N¬∞4</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/80568426-a136-48a6-b219-9c05dd3e2584" width="330" height="230"/>
    </td>
    <td>
    <br> We upgraded the chassis to a modified HiWonder Kit and replaced the infrared sensors with a LiDAR for more accurate obstacle detection. The webcam was also switched to a monocular camera. The original car motor was replaced by two encoder motors, adapted with gears to drive a single wheel in compliance with competition guidelines.<br><br>

For the Ackermann steering, we replaced the stepper motor with a servomotor. On top of that, we moved away from the Arduino Nano and began implementing the ROS framework.<br><br>
<a href="v-photos/vehicle-versions/README.md#version-4">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 5 -->
  <tr>
    <td align="center"><i>Version N¬∞5</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/fe6757a1-3fe5-4b12-b856-bedcc28a5b50" width="330" height="230"/>
    </td>
    <td>
      <br>Our main improvement was the chassis. We joined the two bases by drilling them together and carefully organized the components with the Raspberry Pi inside. The LiDAR was placed on top so nothing would block its view, and we also completed and installed the camera housing.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-5">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 6 -->
  <tr>
    <td align="center"><i>Version N¬∞6</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/7e483e4c-2f35-48e9-8125-00a2123f06a2" width="330" height="230"/>
    </td>
    <td>
      <br> Wheels were changed to adjust the car‚Äôs height so the Lidar could detect walls within the 10 cm range (previously it was too high and failed). The housing material was upgraded from PLA to polycarbonate for greater resistance, and the Open Challenge (autonomous 3 rounds driving) was completed.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-6">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 7 -->
  <tr>
    <td align="center"><i>Version N¬∞7</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/105155b6-ddb2-4885-aa63-7e07f1468315" width="330" height="230"/>
    </td>
    <td>
      <br> Using our HiWonder kit as the base, we designed and cut a completely new, smaller chassis with personalized mounting holes for all components, new housing pieces were created, and the two-motor system was replaced by a single motor in a gear system.<br><br>
    We also moved from a two-level structure to a single-level layout, placing all the components on the same surface to give the Raspberry Pi better airflow and easier access. The car successfully detected and avoided the first traffic signs.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-7">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

  <!-- Version 8 -->
  <tr>
    <td align="center"><i>Version N¬∞8</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/5d66a0cd-a12c-4155-ae6f-aa4655cf6e0e" width="330" height="250"/>
    </td>
    <td>
      <br> <b>Car dimensions:</b> 15 x 23 cm <br><br>
      We 3D-printed new, slimmer front wheels because the original ones stuck out too much from the chassis. A custom housing was also printed for the batteries, and most importantly, the Ackermann steering servo was mounted vertically to save space and allow for a wider turning angle.<br><br>
During previous testing, we realized the LiDAR was struggling to properly detect the walls of the field, so we 3D-printed and implemented a small angled mount to give it a slight tilt.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-8">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>

<!-- Version 9 -->
  <tr>
    <td align="center"><i>Version N¬∞9</i></td>
    <td align="center"><img src="https://github.com/user-attachments/assets/312c2e7e-4638-4917-b54a-fff2bb1a2965" width="330" height="450"/>
    </td>
    <td>
      <br> <b>Car dimensions:</b> 15 x 20 cm <br><br>
In this version, the components were arranged more efficiently to save space. We added a custom housing for the batteries, placed the Pi5 controller on top, and mounted the Raspberry above it, creating a layered system.<br><br>
A new chassis base was printed in MDF, and the Ackermann was moved slightly because, in the previous version, the rack was colliding with the servo. To improve traction, we added a groove to the wheels and printed small cylinders between them to prevent contact with the screws. With these adjustments, the robot managed to complete a lap in <b>20 seconds</b>.<br><br>
      <a href="v-photos/vehicle-versions/README.md#version-9">‚û°Ô∏è See more photos üöó</a><br><br>
    </td>
  </tr>
  
  
</table>

<br>

> [!NOTE]
> Visit our [`vehicle-versions`](v-photos/vehicle-versions/README.md) folder to see photos and videos of our car evolution üîù

<br>


## 3. System Setup
### **<ins>Operating Environment Overview</ins>**
The operating environment of our robotic car is designed as a structure that connects hardware, software, and middleware into a single functional system, shown in the diagram below:

<p align="center">
  <img src="https://github.com/user-attachments/assets/94d8b0e1-d5f1-4a22-9ba9-11ce9cd6ee5d" alt="diagram" width="90%">
</p>



### **<ins>Robot Operating System (ROS)</ins>** <img width="50" alt="ROS" src="https://github.com/user-attachments/assets/53574d65-315e-4dfd-a8d9-ffb38e892bab" />
ROS is a framework that connects a robot‚Äôs software and hardware, making sensors, motors, and programs work together so the robot can perform tasks smoothly. For this project, we use ROS 2 Jazzy, one of the latest versions of ROS 2. You can find the documentation here:  [ROS 2 Documentation: Jazzy](https://docs.ros.org/en/jazzy/index.html)<br>

Without ROS, everything would have to be written in one long, complicated program that‚Äôs hard to manage. We can now divide the system into smaller parts, or nodes, that each do one job, making the system easier to build, fix, and expand.<br>

<p align="center">
  <img src="https://github.com/user-attachments/assets/b94b35c9-c47d-48f0-9a54-57789c4cc455" alt="Nodes" width="80%">
</p>

> This is an official animation to better understand how nodes work. Each node does one clear job, like moving wheels or reading sensors, and they talk to each other using topics, services, actions, or parameters.
>  In practice, this is the process:
> 1. Sensors send data to ROS 2 by publishing it on topics.
> 2. Other nodes listen to that data, process it, and decide what the car should do.
> 3. Finally, a controller node sends commands, making the car move accordingly.

<br>

*<ins>Why we use ROS?</ins>*<br>

<table>
  <tr>
    <td><b>Sensor & Actuator Integration</b></td>
    <td>
      ROS connects all sensors and actuators in one system, ensuring seamless coordination. 
      This gives our autonomous car continuous information about its surroundings 
      and optimizes overall performance.
    </td>
  </tr>
  <tr>
    <td><b>Environmental Perception with LiDAR</b></td>
    <td>
      Most manufacturers of advanced sensors, such as LiDARs, provide an official package to use their hardware with ROS. In the case of the DTOF STL-19P, the manufacturer provides a package that automatically publishes the LiDAR data so it can be processed afterward. Support with Python ROS is fully compatible with Python, which allows for versatile and high-level code development. In addition, being open-source, it has a large community that provides support and assistance for robot development.
    </td>
  </tr>
  <tr>
    <td><b>Debugging & Simulation</b></td>
    <td>
      ROS includes tools to quickly debug the content published on topics. It also provides tools such as RViz, which allows real-time data visualization, and Gazebo, which enables running¬†simulations.
    </td>
  </tr>
  <tr>
    <td><b>Efficient Simulation & Debugging</b></td>
    <td>
      Virtual environments like 
      <a href="https://gazebosim.org/home">Gazebo</a> let us fine-tune parameters before implementation. 
      Tools like 
      <a href="https://docs.ros.org/en/humble/Tutorials/Intermediate/RViz/RViz-User-Guide/RViz-User-Guide.html">RViz</a> 
      allow real-time visualization of sensor data and car status, making debugging much easier.
    </td>
  </tr>
</table>



<br>

### **<ins>Ubuntu</ins>** <img width="60" alt="UBUNTU" src="https://github.com/user-attachments/assets/38f176af-2d07-41f9-bb14-3ef8eb2c0022" />
Ubuntu is a popular, free, open-source operating system based on Linux. It is known for being reliable, flexible, and widely used in both research and industry. In our project, we use Ubuntu 24.04 as the **foundation that runs on the Raspberry Pi**.<br>

Ubuntu manages the Raspberry Pi‚Äôs resources efficiently, ensuring it runs correctly. It provides drivers and compatibility for sensors and external hardware, making integration easier. Ubuntu also allows us to install and run ROS 2 Jazzy and gives us access to important libraries and tools that simplify tasks such as sensor communication and system development.<br>

We use Ubuntu because of its stability and compatibility with ROS 2 Jazzy. Since ROS 2 packages are officially distributed for Ubuntu, using this operating system guarantees that we can easily install and manage the software needed for our car.
<br><br>

### **<ins>Raspberry Pi</ins>** <img width="70" alt="Raspberry" src="https://github.com/user-attachments/assets/6e218c1a-8fe1-47a2-8ae5-1719956508fa" />
The Raspberry Pi is a small computer that works as the brain of our car. It is powerful enough to run Ubuntu, ROS 2, and our algorithms in real time. In our project, we use the Raspberry Pi 5, which you can find here: [Raspberry Pi 5](https://www.raspberrypi.com/products/raspberry-pi-5/)<br>

**üëâ Set up and configuration**<br>

With the [Raspberry Pi Imager](https://www.raspberrypi.com/software/), we flash Ubuntu 24.04 into the microSD card of the Raspberry. After that, we configure the basics like Wi-Fi, SSH for remote access, and hostname. And finally, we install ROS 2 Jazzy by following these steps: [Installation Ubuntu (deb packages)](https://www.google.com/url?q=https://docs.ros.org/en/jazzy/Installation/Ubuntu-Install-Debs.html&sa=D&source=docs&ust=1756316032253380&usg=AOvVaw24eBtKhOAoYRVzp4xh2Rkh)

| Step 1 | Step 2 | Step 3 | Step 4 |
| :--: | :--: | :--: |  :--: |
| <img src="https://github.com/user-attachments/assets/f6ee9449-517c-443a-aad2-c7a4913f8334"/> | <img src="https://github.com/user-attachments/assets/0990b9f5-a0a7-408b-a90b-2e9086ae6032" /> | <img src="https://github.com/user-attachments/assets/8b716aad-e25b-4ff4-a92a-8e5e1b085319" /> | <img src="https://github.com/user-attachments/assets/c0f70529-b1b9-4ce7-b0c2-3803a02c44db"/> |


<br>

## 4. Mobility Management  

### <ins>**Steering System ‚Äì Ackermann**</ins>

<p align="center">
  <img src="https://github.com/user-attachments/assets/8cfcffdb-48aa-4297-a41c-b494a0f222c0" alt="Ackermann" width="70%">
</p>

Our autonomous car uses an Ackermann steering system, controlled by a **15 kg¬∑cm digital servo**, which provides precise and stable control for navigation and turns.  

The Ackermann steering geometry is designed to reduce tire slip by ensuring that all wheels align as radii of circles that share a common center when the car is turning. This configuration keeps the rear wheels fixed and places the center of rotation along a line extended from the rear axle. To achieve this geometry, the inside front wheel turns at a greater angle than the outside front wheel, allowing smoother and more efficient cornering.

---

### **Our own modifications ‚öíÔ∏è**

In the first versions of the car, we implemented the Ackermann steering system using a custom mechanism. We designed and 3D printed a gear connected to a stepper motor, along with a rack, which is a stick with grooves that fit into the gear teeth. When the motor rotated the gear, the rack would move, which in turn rotated the wheels.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/211ba653-b04c-4f56-9fca-0c7498eb9aff" width="300" height="200">
  <img src="https://github.com/user-attachments/assets/e45a7e54-3c88-4524-b427-0ecff04898f5" width="300" height="200">
</p>

One of the challenges we faced was the 3D printing process itself. Printing small details such as gear teeth was difficult and often imprecise, which caused problems in the initial prototypes. To solve this, we made the gear teeth larger, and while this worked mechanically, the final design ended up taking too much space inside the chassis.

For this reason, we decided to switch to the system we currently use, which is part of the **HiWonder kit**, adapted to fit in our chassis base. Instead of gears, it uses a system of linkages connected by screws and supported by bearings. These linkages move and transfer the motion to the wheels, achieving the Ackermann steering effect in a more compact way.

<p align="center">
  <img src="https://github.com/user-attachments/assets/4843c1d0-fb98-4bd4-b9e3-5fe4925295cd" width="300" height="200">
</p>

> [!WARNING]
> At first, the Ackermann of the HiWonder kit worked fine ‚Äî the car could turn and even get through some obstacles. But after a lot of testing, we noticed that whenever the car turned left, the Ackermann didn‚Äôt rotate as much as it did when turning right. This made obstacle avoidance harder, especially in the field corners, so we knew we had to make adjustments.

We realized that the servo needed to be repositioned. First, it was placed horizontally under the Ackermann linkages. We brainstormed how to make it more efficient and changed the position of the servomotor, mounting it vertically with the accessory facing downward under the chassis. This way, the linkages could move freely with more angles, and we also freed up extra space for the other components. This new placement also allowed us to reduce the length of the chassis.  

<table>  
  <tr>
    <th width="30%">Initial position</th>
    <th width="30%">Idea</th>
    <th width="30%">Final Position</th>
  </tr>
  <tr>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/c2211141-81ed-4bbd-ad60-2111033b03cd"/>
    </td>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/fe8e9d70-c1c7-496e-81b4-c050766807a8"/>
    </td>
    <td align="center">
      <img src="https://github.com/user-attachments/assets/85a6c25b-8473-4ee0-b0c3-f3ce70bd4888"/>
    </td>
  </tr>
  <tr>
    <td align="center">Servo mounted horizontally (limited angles)</td>
    <td align="center">Servo repositioned vertically</td>
    <td align="center">Final placement with improved steering</td>
  </tr>
</table>

---

### <ins>**Motor and Drivetrain**</ins>

<p align="center">
  <img src="https://github.com/user-attachments/assets/f636176b-af6a-4b72-bb31-53f025ab41b1" width="70%">
</p>

The drivetrain of our autonomous car is powered by a **25 mm metal gear DC motor**, chosen for its compact size and high torque. The motor is mounted on the chassis and directly connected to the rear axle through a system of gears, ensuring efficient transfer of power to the wheels.  

Our drivetrain includes a gear reduction system:  
- The small gear is mounted on the motor shaft.  
- This small gear drives a larger gear connected to the rear axle.  
- The result is a reduction ratio that increases torque at the wheels, providing more force for acceleration and stability, even if the motor speed itself remains constant.

<p align="center">
  <img src="https://github.com/user-attachments/assets/1b89468f-1234-4610-8944-20d835b95d5b" width="50%">
</p>

During assembly, we noticed a gap between the metal chassis part and the axle supports. This caused instability in the drivetrain. To fix it, we designed custom 3D-printed cylindrical spacers that fill the gap and keep the axle firmly in place. This simple solution reduces vibrations, prevents misalignment, and ensures smoother transmission of power from the motor to the wheels.

<p align="center">
  <img src="https://github.com/user-attachments/assets/a3747319-fd2e-4542-ac6d-be17239430e3" width="50%">
</p>

---

### <ins>**3D Pieces**</ins>

This is the 3D model of our vehicle.  
Below, you will find a table with our 3D-printed parts and their descriptions.

<p align="center">
  <img src="https://github.com/user-attachments/assets/3efc5d12-f77e-447b-8fb1-fdd75fd965ec" width="90%">
</p>

<table align="center" width="100%" style="table-layout: fixed;">
  <thead>
    <tr>
      <th width="30%">Component</th>
      <th width="40%">Preview</th>
      <th width="30%">Folder</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td align="center"><b>Vehicle Base</b></td>
      <td align="center">
        <img src="models/vehicle_base/vehicle_base.png" width="90%">
      </td>
      <td align="center">
        <a href="models/vehicle_base/" target="_blank">View</a>
      </td>
    </tr>
    <tr>
      <td align="center"><b>Vehicle Wheels</b></td>
      <td align="center">
        <img src="models/vehicle_wheels/vehicle_wheels.png" width="90%">
      </td>
      <td align="center">
        <a href="models/vehicle_wheels/" target="_blank">View</a>
      </td>
    </tr>
    <tr>
      <td align="center"><b>Camera Housing</b></td>
      <td align="center">
        <img src="models/camara_housing/camara_housing.png" width="90%">
      </td>
      <td align="center">
        <a href="models/camara_housing/" target="_blank">View</a>
      </td>
    </tr>
    <tr>
      <td align="center"><b>LiDAR Housing</b></td>
      <td align="center">
        <img src="models/lidar_housing/lidar_housing.png" width="90%">
      </td>
      <td align="center">
        <a href="models/lidar_housing/" target="_blank">View</a>
      </td>
    </tr>
    <tr>
      <td align="center"><b>Raspberry Housing</b></td>
      <td align="center">
        <img src="models/raspberry_housing/raspberry_housing.png" width="90%">
      </td>
      <td align="center">
        <a href="models/raspberry_housing/" target="_blank">View</a>
      </td>
    </tr>
  </tbody>
</table>


## 5. Power & Sense Management 

### <ins>**Power Source**</ins>

<p align="center">
  <img src="https://github.com/user-attachments/assets/8d83ffe1-569e-4fe6-a97f-a14b833fa9ac" width="80%">
</p>

Our autonomous car is powered by 7.4V Li-Po batteries (2000mAh, 20C). We chose Li-Po batteries because they provide a high energy density, meaning more power in a small and lightweight package. The battery provides two main energy lines: one for the electronics and another for the motor: <br>

1. **Electronics & Sensors**  
   The battery first connects to the **RRC Lite Controller**, which regulates the power down to a safe **5 V**.  
   From there, it distributes electricity to:  
   - **Raspberry Pi 5** ‚Üí acts as the brain of the car, running ROS 2 and processing sensor data.  
   - **Digital servomotor** ‚Üí controls the Ackermann steering system.  
   - **STL-19P TOF LiDAR & monocular camera** ‚Üí provide vision and distance perception.  

2. **Drive Motor (via L298N Motor Driver)**  
   In parallel, the battery also powers the **L298N motor driver** directly with **7.4 V**.  
   This driver regulates how much current goes to the **25 mm metal DC motor**, which is responsible for moving the car forward.  

By splitting the power into two paths (one regulated for sensitive electronics and one direct for the motor), the system ensures stability. Motors usually demand sudden spikes of current, and separating their supply avoids crashes or interruptions in the Raspberry Pi and sensors.


### <ins>**Sensors Integration**</ins>
To perceive its environment and handle the Future Engineers challenges, the car uses a combination of:  

- **STL-19P TOF LiDAR**: provides 360¬∞ distance data.  
- **2DOF Monocular Camera**: detects colors and obstacles.  

Together, these sensors give the robot a better understanding of its environment by combining depth perception with visual input.


### <ins>**STL-19P TOF LiDAR**</ins>

<p align="center">
  <img src="https://github.com/user-attachments/assets/853f8730-a645-47b5-bacf-ded61b21a6c9" width="80%">
</p>

<br>

Unlike simpler sensors such as ultrasonic or infrared, which measure only in a single direction and have limited precision, the LiDAR sensor is capable of a **360¬∞ scanning** and provides precise distance measurements by using laser pulses. It helps the robot map its surroundings over a wide range, detect obstacles, and navigate more accurately in dynamic environments.

<br>

| Characteristic        | Vaue                        |
|------------------------|------------------------------|
| Ranging distance       | 0.03 ‚Äì 12 m                 |
| Size                   | 38.59 x 38.59 x 34.8 mm     |
| Scanning Angle         | 360¬∞                        |
| Scanning Frequency     | 5 ‚Äì 13 Hz                   |
| Ranging Accuracy       | ¬±45 mm                      |
| Ranging Frequency      | 5000 Hz                     |

> [!IMPORTANT]
> **üìçPlacement:**
> The LiDAR was positioned at an altitude of under 10 cm, so it can detect the walls of the path, which are also 10 cm high. This placement ensures the Lidar has a clear view of the obstacles while remaining unobstructed by other components. All other elements on the car were arranged to avoid blocking the Lidar‚Äôs line of sight. <br><br>
> <img width="1206"  src="https://github.com/user-attachments/assets/9e9d9df4-cfc6-4875-b832-435c4d2ebb1c">

  
### <ins>**2DOF Monocular Camera**</ins>

<p align = "center">
  <img src = "https://github.com/user-attachments/assets/546b6072-3ab9-4a02-b0a2-437478ac0b03" width="50%">
  </p>
<br>

The 2DOF Monocular Camera complements the LiDAR by adding visual perception. This allows the robot to recognize its environment beyond distance data, enabling future applications such as detecting the obstacles‚Äô colors:

 + **Color detection**: Since graphical color detection is a core part of our  project, the camera can precisely identify various colors on the course. This allows our car to interact with the different traffic signs in the Obstacle challenge.
  + **Spatial awareness through data fusion**: When combining camera data with our sensor, the robot gains a richer understanding of its environment. The camera provides detailed visual context that leads to an adaptable navigation strategy.

> [!IMPORTANT]
> **üìçPlacement:**
> The camera is located in a special 3D mounting piece with a certain  inclination angle that points to the floor so it can better detect obstacles. At first it was placed at the front of the car but this location provided a limited vision and sticked out of the chassis lenght. See more detailed info of the housing piece in the [`3D Printed Parts`](#3d-printed-parts) section. <br><br> 
> <img width="1206"  src="https://github.com/user-attachments/assets/a286367c-681d-473b-807f-c9e470b2fb0d">



### **<ins>BOM (Bill of Materials)</ins>**
| Component | Quantity | Description | Image |
|-----------|----------|-------------|-------|
| Raspberry Pi 5 | 1 | Main processing unit for running **ROS**. Acts as the main brain and the Host Controller of the system, capable of running operating systems and handling complex processing tasks. | <img width="850" alt="Raspberry Pi 5" src="https://github.com/user-attachments/assets/ee89c342-6957-4551-b429-e7731c2a28df" /> |
| RRC Lite Controller | 1 | Integrates: ROS expansion board, High-Frequency PID Control, Motor Closed-Loop Control, Servo Control and Feedback, IMU Data Acquisition, Power Status Monitoring, and a Power Switch. | <img width="850" alt="Controller" src="https://github.com/user-attachments/assets/929afeb7-4486-4cd3-bb9e-a8a51bd91e83" /> |
| STL-19P TOF Lidar | 1 | Provides precise, 360-degree distance measurements for real-time navigation and obstacle detection in dynamic environments. | <img width="850" alt="Lidar" src="https://github.com/user-attachments/assets/927d47f4-c856-461d-9e6e-ba1c35318990" /> |
| Lidar Adapter Board | 1 | Converts the LiDAR‚Äôs UART signals to USB for PC connection and data reading. | <img width="850" alt="Adapter" src="https://github.com/user-attachments/assets/407671ef-bed2-4a04-8aef-0cb69cab464a" /> |
| 15 kg.cm Digital Servo | 1 | Provides accurate steering control. | <img width="850" alt="Servo" src="https://github.com/user-attachments/assets/b949c033-fc6d-4032-8d41-b136ea6ddc90" /> |
| 65 mm Anti-Slip Rubber Wheel | 2 | High-grip wheels to improve traction and stability. | <img width="850" alt="Wheel" src="https://github.com/user-attachments/assets/3f50620d-b008-4426-86f5-2c4f5ee6aff6" /> |
| 25MM Metal Gear Motor | 1 | Core drive motor for powering the wheels with torque and speed. | <img width="850" alt="Motor" src="https://github.com/user-attachments/assets/de6786ac-3dc2-4d36-89e3-03435708c338" /> |
| Rubber Tire Edge | 2 | Ensures 3d- printed wheels stability and grip on the floor. | <img width="850" alt="tire" src="https://github.com/user-attachments/assets/840e6304-55cd-4e40-b4e4-76193dc6ad4f" /> |
| Monocular Camera | 1 | A camera used for capturing images and videos, can be used for computer vision or live streaming. | <img width="850" alt="Camera" src="https://github.com/user-attachments/assets/47eeb574-59c4-4cdf-be45-4133167f8739" /> |
| L298N Motor Driver | 1 | Controls motor direction and speed from the Raspberry Pi. | <img width="850" alt="Motor Driver" src="https://github.com/user-attachments/assets/3ab3c2e0-8b73-4996-9412-b28877301502" /> |
| Jumper Cables | 4‚Äì8 | Electrical connections between the motor driver and Pi. | <img width="850" alt="Jumper Cables" src="https://github.com/user-attachments/assets/44758ea5-a6b2-4366-a1fb-7a0eeac5135b" /> |
| USB-USB Cable | 1 | A cable to connect the Raspberry Pi to the camera, Lidar, and Controller. | <img width="850" alt="USB Cable" src="https://github.com/user-attachments/assets/e28195ef-1993-4163-a3de-d2299c443eb0" /> |
| Li-Po Battery 7.4 V 5000mAh 20C | 1 | A lithium polymer battery that provides portable, high-density power. | <img width="850" alt="Battery" src="https://github.com/user-attachments/assets/204ea520-4bc6-4757-93e2-d99036c49403" /> |




### <ins>**Wiring Diagram**<ins>

The following visual wiring diagram illustrates the **physical connections** between the modules (Raspberry Pi, LiDAR, camera, motor, etc.) using **realistic component images**. It helps visualize the system layout and understand **how components are arranged and linked** in the robot.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/42416f9e-0c0a-4751-b446-b05c707c9434" width="80%">
</p>

#### **1. Raspberry Pi 5 Connections**
The Raspberry Pi 5 works as the brain of the car. Its USB ports connect the main sensors and the controller:

+ Camera ‚Üí provides visual input for detecting traffic signs and colored obstacles.
+ LiDAR ‚Üí provides distance and wall detection for navigation.
+ RRC Lite Controller ‚Üí interfaces with the digital servomotor and forwards control signals to the motor driver.

<p align="center">
  <img src="https://github.com/user-attachments/assets/0b2efa85-6dec-49f6-ba25-af40653e8c89" width="80%">
</p>

#### **2. RRC Lite Controller**
The RRC Lite Controller acts as a bridge between the Raspberry Pi and the actuators. It regulates incoming power from the Li-Po battery (down to a safe 5 V) and distributes it to:  
- The **Raspberry Pi 5**  
- The **digital servomotor** for Ackermann steering  

It also handles communication with the motor driver to send PWM control signals.

---

#### **3. Motor Driver and DC Motor**
The **L298N Motor Driver** is powered directly by the **7.4 V Li-Po battery**.  
- It regulates both the **speed** and **direction** of the **25 mm metal gear DC motor**.  
- The motor then powers the rear axle through a gear reduction system, increasing torque for smoother acceleration.  

---

#### **4. Power Source**
The **7.4 V Li-Po battery** supplies energy to the entire system, split into two paths:  
- **Direct line (7.4 V):** powers the L298N motor driver and DC motor.  
- **Regulated line (via RRC Lite Controller):** delivers 5 V to the Raspberry Pi, servomotor, and sensors.  

<br>

This is also a diagram that shows the **electrical wiring** and pin-level connections between all components ‚Äî including power lines, GPIOs, and communication ports. It focuses on **signal paths and voltage levels** rather than physical appearance.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/6e7abe3c-c553-48f9-9c59-543b3cd9996b" width="80%">
</p>

> [!NOTE]
> Visit our [`schemes`](https://github.com/vania020/wro2025-robotek/tree/main/schemes) folder to access all of our diagrams and schemes üîëüöó

## 6. Obstacle Management 

### <ins>**Control Node Structure**</ins>

Since we use ROS 2 as the middleware that connects all the components of our autonomous vehicle, there is no single ‚Äúmain‚Äù code that runs either the Open Challenge or the Obstacle Challenge. Instead, the system is built as a collection of independent but interconnected ROS 2 nodes, each performing a specific function such as reading the camera, processing LiDAR data, or controlling the motors. These nodes communicate constantly through topics and messages, allowing the car to behave as a cohesive, intelligent system.

You can access our node codes here:
<br>

| Node | Code Description | Function |
|------|------|-----------|
| üõ∞Ô∏è Acker LiDAR Node | [`acker_lidar_node`](./src/acker_lidar_node) | Reads and processes LiDAR scans. |
| üé• Camera Node | [`camara`](./src/camara) | Detects obstacles and colors. |
| ‚öôÔ∏è Motor Node | [`motor`](./src/motor) | Sends PWM signals to control wheel speed. |



<p align="center">
  <img src="https://github.com/user-attachments/assets/52664f04-fa23-48df-a97d-d70433b9a828" width="80%">
</p>

This table better explains our ROS topics and messages:

| **Node**                                | **Role**           | **Topic**                      | **Message Type**                                                   | **What It Transmits**                                                                                                                       |
| --------------------------------------- | ------------------ | ------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **LiDAR Node**                          | Publisher          | `/scan`                        | `sensor_msgs/LaserScan`                                            | It sends 360¬∞ distance readings from the LiDAR; each value represents how far an obstacle is at a given angle.                                 |
| **Camera Node**                         | Publisher          | `/obstaculos`                  | `std_msgs/String`                                                  | It publishes the detected obstacle color in a text form: (`rojo`, `verde`, or `ninguno`), which defines how the main controller adjusts the steering setpoint. |
| **AckerLidar Node** *(Main Controller)* | Central Controller | `/motor_vel`, `/positionServo` | `std_msgs/Float32`, `ros_robot_controller_msgs/SetAckerServoState` | `/motor_vel`: Motor duty cycle (speed %). `/positionServo`: Steering position (PWM in ¬µs) for Ackermann control.                            |
| **Motor Node**                          | Subscriber         | `/motor_vel`                   | `std_msgs/Float32`                                                 | Receives motor velocity commands from the main controller and translates them into PWM signals for the DC motor driver (L298N).             |
| **Raspberry Pi 5 Controller Node**      | Subscriber         | `/positionServo`               | `ros_robot_controller_msgs/SetAckerServoState`                     | Executes servo position commands and moves the front wheels to reach the target angle.                                                      |
| **RRC Controller Button**               | Publisher          | `/button`                      | `ros_robot_controller_msgs/ButtonState`                            | Sends an activation signal when the onboard button is pressed, starting the robot‚Äôs control loop.                                           |

### **What is a Message type?**

A **message type** is like a template that defines **what kind of data** is sent through a *topic*. For example, ROS already includes many built-in types, such as:

* `std_msgs/String` ‚Üí used to send text.
* `std_msgs/Float32` ‚Üí used to send a decimal number.
* `sensor_msgs/LaserScan` ‚Üí used to send LiDAR data (distance readings around 360¬∞).
* `geometry_msgs/Twist` ‚Üí used to send linear and angular velocities (very common in mobile robots).
* And you can also have **custom message types**, like the ones included in your own package `ros_robot_controller_msgs`.

### **Important clarifiactions**

* For simplicity, **some topic names were shortened** in the diagrams.

  * The real ROS topic `/ros_robot_controller/acker_servo/set_state` is represented as **`/positionServo`**.
  * The real ROS topic `/ros_robot_controller/button` is represented as **`/button`**.

* The button input `/ros_robot_controller/button` is processed internally by the main node and does not appear as a separate node because it doesn‚Äôt exchange messages with others.

* The naming convention was simplified only for documentation clarity; all logic and connections in the code remain unchanged.

* The **AckerLidar Node** acts as the core controller that links everything:
  it subscribes to LiDAR and camera data, processes the PID control, and publishes both the steering and velocity commands.



> [!NOTE]
> If you would like to see a detailed description and explanation of the code behind each node, please visit our [`src`](https://github.com/vania020/wro2025-robotek/tree/main/src) folder üñ•Ô∏èüöó
> 

---


### <ins>**Open Challenge**</ins>

The **Open Challenge** is the first autonomous driving test. In this stage, the robot must complete three laps **without any external input** relying only on its onboard **LiDAR sensor**, **PID controller**, and **Ackermann steering system**. The goal is to keep the car centered between the inner and external walls of the challenge during the entire lap. To achieve that, the robot constantly measures two key distances: **D‚ÇÅ** ‚Üí Distance from the car to the **left wall** and **D‚ÇÇ** ‚Üí Distance from the car to the **right wall**

The **control goal** is defined by the equation:

> **D‚ÇÅ - D‚ÇÇ = 0**  
> *(Setpoint = 0 ‚Üí car is centered)*

When this balance holds true, it means that both walls are equidistant, and the car is aligned in the center of the track.

<p align="center">
  <img src="https://github.com/user-attachments/assets/672a9f3c-875b-4f9e-a2d6-b38b62be8fdf" width="80%">
</p>

### **How It Works: LiDAR + Control Loop**

The LiDAR sensor scans the surrounding environment in real time, detecting obstacles and measuring the distances around the car. From these scans, two zones are analyzed, one on the **left** and one on the **right** to extract D‚ÇÅ and D‚ÇÇ.

These values are sent to the **AckerLidarNode**, the main control node in charge of:
- Reading and processing LiDAR data  
- Computing the distance difference `error = D‚ÇÅ - D‚ÇÇ`  
- Sending control signals to the **steering** and **motor** nodes

The logic is the following:  
1. If **D‚ÇÅ > D‚ÇÇ**, the car is too close to the right wall ‚Üí it turns slightly **left**.  
2. If **D‚ÇÅ < D‚ÇÇ**, the car is too close to the left wall ‚Üí it turns slightly **right**.  
3. If **D‚ÇÅ = D‚ÇÇ**, the car is centered ‚Üí it continues straight.

This process runs in a **PID feedback loop**, where:
- The **P (Proportional)** term corrects small deviations quickly.  
- The **I (Integral)** term reduces long-term bias (not always needed).  
- The **D (Derivative)** term prevents oscillations and overshoot.  

The result is a **smooth, stable trajectory** that keeps the car aligned throughout the race.

<div align="center">
  <img src="https://github.com/user-attachments/assets/0b9e80ab-a3b6-4cbc-9457-b2b9e9251c7a" width="80%">
</div>

### **Open Challenge Flowchart**

<p align="center"> <img src="https://github.com/user-attachments/assets/ed4b525c-7d0a-49ee-9f34-ae5922455ce9" width="80%"> </p> 

#### Step-by-Step Description

1. **Start Robot & Initialization**  
   ROS2 nodes are launched: the LiDAR begins scanning, and the control node initializes all parameters (PID gains, setpoint, motor topics).

2. **Continuous Loop**  
   The system enters a continuous loop (`while rclpy.ok()`), running dozens of times per second. Each cycle updates sensor readings and steering actions.

3. **LiDAR Scan Environment**  
   The sensor performs a 360¬∞ scan to detect the walls and extract points on both sides of the track.

4. **Extract Wall Distances (D‚ÇÅ, D‚ÇÇ)**  
   The algorithm filters the LiDAR data to isolate the left and right regions, computes the average distance for each, and updates D‚ÇÅ and D‚ÇÇ.

5. **Compute Error (D‚ÇÅ - D‚ÇÇ)**  
   The difference between these distances represents how ‚Äúoff-center‚Äù the car is from the ideal middle of the lane.

6. **PID Steering Adjustment**  
   The PID controller processes this error and outputs an angle correction, which is sent to the **servo motor** using an Ackermann steering model.

7. **Update Lap Counter**  
   The control node counts laps based on internal flags or distance traveled (depending on the implementation in the ROS2 package).

8. **3 Laps Completed ‚Üí Stop Vehicle**  
   After completing three full laps, the system safely reduces speed and stops the motor node.

### **Nodes and Communication**

During the Open Challenge, three ROS2 nodes work together in real time:

| Node | Role | Description |
|------|------|-------------|
| **`AckerLidarNode`** |Main Control Node | Subscribes to LiDAR data, calculates the distance difference, runs the PID controller, and sends steering/motor commands. |
| **`MotorPWMNode`** | Motor Control | Receives speed values (`Float32`) and controls the DC motor through PWM signals, ensuring smooth acceleration. |
| **`SetAckerServoState`** | Steering Control | Adjusts the steering servo angle according to PID output, maintaining Ackermann kinematics. |


---


### <ins>**Obstacle Challenge**</ins>

<p align="center">
  <img src="https://github.com/user-attachments/assets/709c3f14-f9d9-4626-b078-1e8304557f87" width="100%">
</p>

### **Obstacle Challenge Flowchart**

<p align="center">
  <img src="https://github.com/user-attachments/assets/def1b241-3253-4e54-824d-86904a846773" width="80%">
</p>


<br><br>
## 7. Assembly Instructions 

<br><br>
## 8. Performance Videos 

<div align="center">

| Challenge | YouTube Video |
|------------|----------------|
| **Open Challenge** | [Watch on YouTube üé•](https://www.youtube.com/watch?v=iRfN6pBE0Ac) |
| **Obstacle Challenge** | [Watch on YouTube üé•](https://www.youtube.com/watch?v=5Ic2xejqBcA) |

</div>









